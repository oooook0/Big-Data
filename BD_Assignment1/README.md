# Assignment A 

## SCRUB.py (seperate the signal and noise from the data.txt) 

### Instruction
* The SCRUB.py is composed of three parts: reading, processing and outputing.
  * Reading         
      
         I use ```Read_at``` to load the file to deal with the situation where the file size is like many GB. Before reading the data, I divide the file into several chunks and calculate size_per_process according to the memory free and the number of processes. And the whole process is done by ```while```. Note that size_per_process is not only controlled by the variables mentioned above. It is also adjusted according to the file size itself in order not to output much useless information.    
         
     ```
     if counter * read_size + rank * size_per_process + size_per_process <= file_size:
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     elif counter * read_size + rank * size_per_process > file_size:
         buffer_per_process = bytearray(0)
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     else:
         buffer_per_process = bytearray(file_size - counter * read_size - rank * size_per_process)
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     ```
     
   * Processing      
    
         There are some rules in order to seperate the signal and noise. Note that all the filtering does not involve type conversion, which can save a lot of time.
                
        * -1293.42,-207056-----> Invalid price/volume. Can't have negative.
        
        * 2014O804:10:00:13.826301 ----> Invalid date. Non-numeric character.
        
        * remove duplicated information
        
        * remove abnomal large or small price.
             
   * Output
   
         I use ```Write_ordered``` to output my signal and noise. The reason is that it can control the write order which is extremely important in multithreading compiling.
   
* log file can be found, which will provide you the information of status of compliling and time consumption.
  
* Running in the terminal:

      ```
      mpirun -np 4 python SCRUB.py -file data-big.txt
      ```

## Normal.py (test signal.txt generated by the previous problem) 
### Instruction
* The SCRUB.py is composed of two parts: reading and processing.
  * Reading (same as before)        
      
         I use ```Read_at``` to load the file to deal with the situation where the file size is like many GB. Before reading the data, I divide the file into several chunks and calculate size_per_process according to the memory free and the number of processes. And the whole process is done by ```while```. Note that size_per_process is not only controlled by the variables mentioned above. It is also adjusted according to the file size itself in order not to output much useless information.    
         
     ```
     if counter * read_size + rank * size_per_process + size_per_process <= file_size:
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     elif counter * read_size + rank * size_per_process > file_size:
         buffer_per_process = bytearray(0)
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     else:
         buffer_per_process = bytearray(file_size - counter * read_size - rank * size_per_process)
         in_file.Read_at(counter * read_size + rank * size_per_process, buffer_per_process)
     ```
     
   * Processing      
    
         I select the earlist time in one minute as the time slot after the whole data is locally sorted. The returns can be calculated and gathered to the root. Use ```describe()``` and ```normaltest()``` to get the primary characteristics of the data and the normality of the time series. The results are saved in the log file. Note that there is one commented part in NORMAL.py. This part aims at drawing a histgram of returns. In order to adapt to the HPCC environment, it is commented.

* Running in the terminal:

      ```
      mpirun -np 4 python NORMAL.py -file signal.txt
      ```


## Run on PENZIAS

use ```git``` and ```wget``` to get the code and the data on the scratch. submit the pbs file.
